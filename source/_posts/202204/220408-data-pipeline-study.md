---
title: 220408 데이터 파이프라인 스터디 1일차
date: 2022-04-08 16:04:00
tags:
  - BigData
  - Data-Pipeline
categories:
  - Data-Pipeline
# hidden: true
# secret: true
---

<div align="center">
  <img src="/images/post_images/220408_data-pipeline.webp" alt="Data Pipeline">
</div>

이번 포스팅에서는 1일차에 학습했던 데이터 파이프라인과 관련된 학습내용을 정리하면서 간단하게 회고 내용을 작성해보려고 한다.

1일차 학습했던 학습내용을 개괄적으로 정리하자면, 우선 `데이터 파이프라인의 흐름을 각 STEP별로 나눠서 이해`하고, `데이터 파이프라인에 필요한 AWS서비스에 대해서 이해` 그리고 `데이터 파이프라인 구성시에 고려해야될 고려사항`에 대해서 학습했다.

학습을 하면서, 모르는 용어가 많이 나왔기 때문에 실제 실무에서 일을 할때 같은 엔지니어들과 소통을 하기 위해서는 이 용어가 매우 중요하기 때문에 용어 공부를 위해 별도의 포스팅에 모르는 용어가 나올때마다 추가를 해가면서 학습을 이어가고 있다.

1일차 학습하면서 느낀점은 생각보다 어렵지만, 이전에 파이프라인이라는 말만 듣고 "뭐지?" 했을때의 막연함은 많이 사라진 것 같아서 기분은 좋다. 무언가 새로운 기술을 배울때 마냥 쉽기만 하면, 다른 사람들에게도 진입장벽이 낮다는 것을 의미하는 것이기 때문에 경쟁력을 갖추기 위해서는 어려운 기술적인 부분도 커버할 수 있는 엔지니어가 되려면 열심히 배워야겠다고 느꼈다.

아무튼 각설하고, 이전에 데이터 엔지니어 채용공고를 많이 찾아보았는데, 공고에서 `AWS 클라우드 환경에서의 개발/운영 경험`에 대한 요구사항이 많았다. 왜 인가 생각을 하면서 학습을 해보았는데 이 궁금증이 해결이 되었다.
간단히 말하면, 온프레미스(On-premise)방식과 클라우드 환경(Off-premise)에서의 데이터 처리가 차이가 있는데, 요구사항이 많다는 것은 `온프레미스 방식보다는 클라우드 환경에서의 데이터 처리가 훨씬 더 효율적이고 좋다는 것을 의미`한다. 어떤 부분이 좋은지에 대해서는 자세히 포스팅의 내용에서 다뤄보도록 하겠다.

<!-- more -->

## <ins><b>(1) 데이터 파이프라인의 흐름의 이해</b></ins>

파이프라인은 하나의 데이터 처리 단계의 출력이 다음 단계의 입력으로 이어지는 형태로, 서로 파이프가 연결된 것과 같은 연결구조로 생각하면 된다.

`[1단계] 데이터 수집` : 이 데이터가 왜 중요한지? 요구사항 수집 및 데이터 선정

`[2단계] 데이터 전처리(Transformation) 저장` : 데이터 수집 단계에서 취득한 데이터를 전처리하고 저장하는 과정

`[3단계] 데이터 시각화 분석` : 전처리 후 저장된 데이터를 SQL이나 데이터 시각화 지식이 없는 다른 부서에 제공을 해주기 위한 데이터 시각화 과정

## <ins><b>(2) 데이터 파이프라인에 필요한 AWS 서비스에 대한 이해</b></ins>

앞서 이미 정리를 했지만 온프레미스 방식의 서버 운영보다는 AWS와 같은 클라우드 기반의 서버 운영방식(off-premise)이 훨씬 장점이 많다고 했다. 그렇다면, AWS 서비스를 기반으로 데이터 파이프라인을 구축했을 때, 많은 AWS 서비스 중에서 어떤 서비스를 선택해서 사용해야되고, 해당 서비스가 어떤 기능을 제공하는지에 대한 기반 지식이 필요하다.
이번 파트에서는 AWS 기반으로 데이터 파이프라인을 구축했을 때, 데이터 파이프라인의 각 세션(수집/전처리 및 저장/분석 및 시각화)에서 AWS의 어떤 서비스를 적용해야되는지에 대해 정리를 해보려고 한다.

### **데이터의 원천**

우선 데이터를 수집하기 위해서는 데이터가 나오는 곳이 있어야한다. 각 기업마다 사용되는 외부 분석 서비스도 있고, 기업에서 운영되는 서비스의 기반 플랫폼이 되는 앱 또는 웹 사이트에 의해서 발생하는 이벤트를 통해서 데이터가 생성되어 이를 통해 데이터를 취득할 수 있다.

### **[1단계] 데이터 수집 단계 :**

우선 첫 번째, 데이터 수집 단계에는 아래의 네 가지 AWS 서비스가 있다.

- Amazon Kinesis Streams
- Amazon Kinesis Firehose
- Amazon API Gate wway
- Lambda function

`우선 첫 번째 Amazon Kinesis Streams은` 일종의 큐 서비스이며, 레디스 큐(=스트림 큐))라고도 한다. 스트림은 흐름의 의미로, 강물에 비유하기도 하며 앞단부터 차근차근 쌓아놓고 흘러가는 데이터의 형태라고 이해하면 된다.
강물에서 물고기를 그물을 사용해서 잡는다고 가정했을 때, 강물이 바로 분석하고자 하는 데이터이고, 물고기가 바로 내가 찾고자하는 데이터이며, 던진 그물의 크기를 윈도우라고 한다.

데이터 전체를 일괄적으로 저장할 수 없기 때문에 폭포가 있는 강처럼 일정 사이즈 용량으로 한정하고, 일정 시간이 되면, 데이터가 소멸하게 되는 형태로 구성된다.

데이터를 스트림 형태로 사용하는 이유는 대량의 데이터를 가지고 있기 때문에 여러 서비스에서 분석 가능하며, 만약 앱상에서 발생한 데이터를 클라우드에 저장하거나 한다면, 사용자가 앱 내에서 네이게이션을 하고 있을 때 앱의 퍼포먼스에 영향을 줘서 버벅거리거나 하는 사용감에 불편함을 줄 수 있다.
앞서 언급한 단점으로 인해서 스트림 형태로 데이터를 사용한다.

수집된 데이터 -> RDBMS에 직접 저장하게 되면, RDBMS는 리소스가 한정적이기 때문에, 트래픽이 일괄적으로 몰리면 앱이 느려지는 경향이 있다.
따라서 이러한 경우에는 수집된 데이터 -> (큐 서비스) -> RDBMS의 형태로 RDBMS에 데이터를 적재하기 전에 큐 서비스가 사이에서 모든 데이터를 받고, 사용자가 네비게이션을 할 때 불편함이 없도록 원활하게 앱이 동작할 수 있도록 도와준다.

`두 번째로 Amazon Kinesis Firehose`라는 큐에 있는 데이터를 별도의 코딩없이 S3에 저장할 수 있도록 지원하는 서비스가 있다. (요즘에는 S3 뿐 아니라 ElasticSearch와 같은 third party에 실시간으로 데이터를 넣을 수 있는 스토리지가 있다)

`세 번째로 Amazon API Gate way`라는 외부에서 웹/앱에서 AWS와 연결시켜주는 역할을 해주는 프록시 서비스가 있다. 각 서비스간에 서로 영향을 주지 않는 마이크로 서비스라고도 하며, 10개의 서비스 중에 1개의 서비스가 fail out이 되더라도 나머지 9개가 정상적으로 동작할 수 있도록 도와주는 서비스이다.

`마지막으로 네 번째, Lambda function`이있다. Lambda function은 Event-driven 데이터 처리를 할때 이벤트에서 발생하는 여러가지 서비스를 핸들링하기 위해서 사용된다.

초기에는 Amazon kinesis Stream -> (Lambda function) -> S3 / RDBMS의 형태로 해서 데이터를 저장하였다.

### **[1.5단계] 데이터 수집과 전처리의 교차점 : AWS Glue**

이 과정에는 AWS Glue라는 서비스가 있다.
우선 AWS Glue의 등장배경은 AWS의 Pipeline이라는 서비스이다. 데이터 파이프라인을 통해서 기본적인 ETL(Extract/Transformation/Load)라는 처리를 하게 되는데, Amazon의 Pipeline이라는 녀석이 관리되는 파이프라인이 많아졌을 때 관리 및 운영하는데 어려움이 있었기 때문에 이러한 문제를 개선하기 위해 등장한 친구가 바로 AWS Glue이다. AWS Glue에는 파이프라인의 기본적인 서비스들이 추가가 되어있으며, 가장 비용 효율적으로 잘 활용되고 있는 부분은 바로 메타 스토어 정보가 포함되어있는 부분이다. 이 메타 스토어 정보에는 데이터 위치/포멧/버전의 변경사항등에 대한 정보를 포함하고 있다.

### **[2단계] 전처리 및 저장 : Amazon EMR, Amazon S3**

Amazon EMR는 AWS에 있는 Hadoop Eco System을 가지고 있는 관리형 서비스이다.
과거에는 하둡을 설정하려면 서버에서 리눅스 설치, 하둡에 필요한 라이브러리 및 서비스를 설치하고 각 노드들을 연결하기 위해 네이밍을 하는 작업들을 해야만 했는데, 대략 2주정도 소요가 된다고 한다.
그런데 이 Amazon EMR을 이용하면, 기본적으로 구축된 환경을 제공해주기 때문에 최대 5-10분 사이에 구축된 서비스를 이용할 수 있다.
Amazon EMR을 통해 전처리하고 전처리된 데이터를 저장하고자하는 타겟 요소에 저장할 수 있도록 도와준다.

### **[3단계] 분석 및 시각화 : AWS Athena, Tableau, Periscope Data, Superset**

AWS Athena는 adhoc하게 데이터를 분석할 수 있도록 도와준다.
그외의 시각화 툴인 녀석들을 이용해서 SQL관련 지식이 없는 타 부서 업무 담당자에게 시각화를 해서 정보를 제공해줄 수 있다.

앞서 정리한 [1단계] ~ [3단계]에서 정리한 내용은 틈틈이 실습을 하면서 내가 지금 하고있는 실습이 몇 단계에 해당되는 내용인지 확인을 하고 실습 및 활용을 하는 것이 좋다.

## <ins><b>(3) 데이터 파이프라인을 구축할 때 고려해야되는 사항</b></ins>

요즘 기업에서 데이터를 활용해서 현재 운영중인 서비스를 개선하는 경우가 많다. 그렇기때문에 데이터 파이프라인 구축에 있어 절대적으로 우선 `지속적이고 에러없는 상태`여야하며, 요구사항에 맞게 가능한한 `빠르게 대응`해야한다. 데이터가 갑작스럽게 많아지는 경우가 생겨서 과부하가 생기는 경우도 있기 때문에 시스템적으로 이러한 문제에 대해 유연하게 `Scability`가 있어야 한다. 즉, `Scale up`과 `Scale Out`이 자유로워야 한다는 의미이다.
`이러한 부하는 이벤트성 데이터에서 많이 발생`하게 되는데, 이벤트 관련 푸시 알림 발송에서는 일괄적으로 발송되기 때문에 복합적으로 다양한 `이벤트들이 발생하는 경우를 대비`하여야 한다.
수집되는 데이터 또한 분석에 유연하도록 `분석하기 쉬운 format`으로 관리되어야하며, JSON 포멧이 많이 활용되고 있다.
